{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets\n",
    "\n",
    "This is a sample code to assist you with vectorising the 'Train' dataset for your assignment 2.\n",
    "\n",
    "First we read the CSV datafiles (Train and Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n",
    "\n",
    "x = train_data.text\n",
    "y = train_data.sentiment\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating instance and label for Train\n",
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "Y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "\n",
    "print(X_train_raw[3])\n",
    "\n",
    "print(len(Y_train))\n",
    "print(len(X_train_raw))\n",
    "\n",
    "# data cleaning\n",
    "#X_train_raw = train_data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "#X_train_raw = X_train_raw.apply(lambda x: re.sub(r'@[A-Za-z0-9]+', '', x))\n",
    "#X_train_raw = X_train_raw.apply(lambda x: re.sub(r'RT[\\s]]+', '', x))\n",
    "\n",
    "#X_train_raw = X_train_raw.apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)]))\n",
    "\n",
    "\n",
    "#check the result\n",
    "print(\"Train length:\",len(X_train_raw))\n",
    "print(X_train_raw[3])\n",
    "\n",
    "#separating instance and label for Test\n",
    "X_test_raw = [x[0] for x in test_data[['text']]]\n",
    "\n",
    "#check the result\n",
    "print(\"Test length:\",len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more info on training dataset\n",
    "\n",
    "print(Y_train[:30])\n",
    "def convert_class(raw):\n",
    "    if raw == 'negative': return 0\n",
    "    elif raw == 'neutral': return 1\n",
    "    elif raw == 'positive': return 2\n",
    "\n",
    "\n",
    "Y_train_num = []\n",
    "for y in Y_train:\n",
    "    Y_train_num.append(convert_class(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_raw, Y_train, test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross fold validation to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf Vectorizer\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)\n",
    "\n",
    "model_lr = LogisticRegression(random_state=0, multi_class='ovr', solver = 'liblinear')\n",
    "model_lr.fit(tf_x_train,y_train)\n",
    "print(model_lr.score(tf_x_test, y_test))\n",
    "\n",
    "mnb = MultinomialNB().fit(tf_x_train,y_train)\n",
    "print(mnb.score(tf_x_test, y_test))\n",
    "\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Tarin dataset using TFIDF\n",
    "#X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "#Use the feature set (vocabulary) from Train to vectorise the Test dataset \n",
    "#X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW + Kbest\n",
    "BoW_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_BoW = BoW_vectorizer.fit_transform(X_train)\n",
    "X_test_BoW = BoW_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "kbest = SelectKBest(chi2, k=1000).fit(X_train_BoW, y_train)\n",
    "kbest_train_X = kbest.transform(X_train_BoW)\n",
    "kbest_test_X = kbest.transform(X_test_BoW)\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Tarin dataset using BoW\n",
    "#X_train_BoW = BoW_vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "#Use the feature set (vocabulary) from Train to vectorise the Test dataset \n",
    "#X_test_BoW = BoW_vectorizer.transform(X_test_raw)\n",
    "\n",
    "\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(kbest_train_X,y_train)\n",
    "\n",
    "print(clf.score(kbest_test_X, y_test))\n",
    "\n",
    "model_lr.fit(kbest_train_X,y_train)\n",
    "print(model_lr.score(kbest_test_X, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the created vocabulary for the given dataset in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = BoW_vectorizer.vocabulary_\n",
    "output_pd = pd.DataFrame(list(vocab_dict.items()),columns = ['word','count'])\n",
    "print(output_pd.shape)\n",
    "\n",
    "vocab_dict\n",
    "output_pd.T.to_csv('BoW-vocab.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from workshop 9\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_mi = mi.fit_transform(X_train_BoW,y_train)\n",
    "X_test_mi = mi.transform(X_test_BoW)\n",
    "\n",
    "models = [#GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None),\n",
    "          LogisticRegression(max_iter = 1000),\n",
    "          SVC(kernel='rbf', gamma=0.7),\n",
    "          SVC(kernel='poly', degree=3)]\n",
    "titles = [#'GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression',\n",
    "          'SVM with a cubic kernel',\n",
    "          'SVM with an RBF kernel']\n",
    "\n",
    "k = 1000\n",
    "\n",
    "for k in [100,1000,10000]: #10,\n",
    "    print('\\n--------------------------------------- K = ', k,'------------------------------')\n",
    "    x2 = SelectKBest(chi2, k=k)\n",
    "    x2.fit(X_train_BoW,y_train)\n",
    "    X_train_x2 = x2.transform(X_train_BoW)\n",
    "    X_test_x2 = x2.transform(X_test_BoW)\n",
    "\n",
    "\n",
    "    mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    mi.fit(X_train_BoW,y_train)\n",
    "    X_train_mi = mi.transform(X_train_BoW)\n",
    "    X_test_mi = mi.transform(X_test_BoW)\n",
    "\n",
    "\n",
    "    Xs = [(X_train_BoW, X_test_BoW), (X_train_x2, X_test_x2), (X_train_mi, X_test_mi)]\n",
    "    X_names = ['complete', 'x2', 'mi']\n",
    "    for title, model in zip(titles, models):\n",
    "        print('\\n=========',title, '(with k=',k,'features): ')\n",
    "        \n",
    "        for X_name, X in zip(X_names, Xs):\n",
    "            X_train_t, X_test_t = X\n",
    "            \n",
    "            model.fit(X_train_t.todense(), y_train)\n",
    "            y_test_predict = model.predict(X_test_t.todense())\n",
    "            accuracy =  accuracy_score(y_test, y_test_predict)\n",
    "            print(X_name, 'accuracy is:',  accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------- K =  10 ------------------------------\n",
    "\n",
    "========= GNB (with k= 10 features):\n",
    "complete accuracy is: 0.4204250114661367\n",
    "x2 accuracy is: 0.26968353462773276\n",
    "mi accuracy is: 0.2761045711664883\n",
    "\n",
    "========= MNB (with k= 10 features): \n",
    "complete accuracy is: 0.6584620088671457\n",
    "x2 accuracy is: 0.592722825256077\n",
    "mi accuracy is: 0.5982265708607246\n",
    "\n",
    "========= one-r (with k= 10 features): \n",
    "complete accuracy is: 0.5965448708148601\n",
    "x2 accuracy is: 0.5965448708148601\n",
    "mi accuracy is: 0.5965448708148601\n",
    "\n",
    "========= 5-nearest neighbour (with k= 10 features): \n",
    "complete accuracy is: 0.5863017887173215\n",
    "x2 accuracy is: 0.5852316159608623\n",
    "mi accuracy is: 0.5849258523161596\n",
    "\n",
    "========= Decision Tree (with k= 10 features): \n",
    "complete accuracy is: 0.5685674973245681\n",
    "x2 accuracy is: 0.6116801712276411\n",
    "mi accuracy is: 0.6112215257605871\n",
    "\n",
    "========= Logistic Regression (with k= 10 features): \n",
    "complete accuracy is: 0.6606023543800642\n",
    "x2 accuracy is: 0.6144320440299649\n",
    "mi accuracy is: 0.6141262803852622\n",
    "\n",
    "========= SVM with a cubic kernel (with k= 10 features): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Adapting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC().fit(tf_x_train, y_train)\n",
    "\n",
    "#cvscvore(svc, tf_x_train, y_train, cv=5). mean0, sve.score (tf_x_test, y_test)\n",
    "print(len(y_train))\n",
    "svc_pred = svc.predict(tf_x_test)\n",
    "print(classification_report(y_test, svc_pred))\n",
    "matrix_svc = confusion_matrix(y_test, svc_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(matrix_svc, cmap = 'Blues', annot = True)\n",
    "plt.xlabel (\"Predicted classes\")\n",
    "plt.ylabel (\"Actual classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(kbest_train_X,y_train)\n",
    "\n",
    "print(clf.score(kbest_test_X, y_test))\n",
    "\n",
    "model_lr.fit(kbest_train_X,y_train)\n",
    "print(model_lr.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "# store best combination as x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "bagging = BaggingClassifier().fit(kbest_train_X, y_train)\n",
    "print(bagging.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "#voting\n",
    "base_classifiers = [('dt', DecisionTreeClassifier()),\n",
    "                    (' 1r', LogisticRegression (C= 0.5)),\n",
    "                    ('mnb' , MultinomialNB()),\n",
    "                    (' svc', LinearSVC (C=1))]\n",
    "voting = VotingClassifier (estimators=base_classifiers)\n",
    "\n",
    "voting.fit(kbest_train_X ,y_train)\n",
    "print(voting.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "#random forest\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators=100, verbose=1).fit(kbest_train_X, y_train)\n",
    "print(rf.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "#ada\n",
    "ada = AdaBoostClassifier().fit(kbest_train_X,y_train)\n",
    "print(ada.score(kbest_test_X, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}