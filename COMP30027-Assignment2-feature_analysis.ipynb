{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets\n",
    "\n",
    "This is a sample code to assist you with vectorising the 'Train' dataset for your assignment 2.\n",
    "\n",
    "First we read the CSV datafiles (Train and Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     12659\n",
      "positive     5428\n",
      "negative     3715\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read data from csv file\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n",
    "\n",
    "x = train_data.text\n",
    "y = train_data.sentiment\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"prince george reservist who died saturday just wanted to help people, his father tells @cbcnews http://t.co/riauzrjgre\"\t\n",
      "21802\n",
      "21802\n",
      "Train length: 21802\n",
      " \"prince george reservist who died saturday just wanted to help people, his father tells @cbcnews http://t.co/riauzrjgre\"\t\n",
      "Test length: 1\n"
     ]
    }
   ],
   "source": [
    "#separating instance and label for Train\n",
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "Y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "\n",
    "print(X_train_raw[3])\n",
    "\n",
    "print(len(Y_train))\n",
    "print(len(X_train_raw))\n",
    "\n",
    "# data cleaning\n",
    "#X_train_raw = train_data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "#X_train_raw = X_train_raw.apply(lambda x: re.sub(r'@[A-Za-z0-9]+', '', x))\n",
    "#X_train_raw = X_train_raw.apply(lambda x: re.sub(r'RT[\\s]]+', '', x))\n",
    "\n",
    "#X_train_raw = X_train_raw.apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)]))\n",
    "\n",
    "\n",
    "#check the result\n",
    "print(\"Train length:\",len(X_train_raw))\n",
    "print(X_train_raw[3])\n",
    "\n",
    "#separating instance and label for Test\n",
    "X_test_raw = [x[0] for x in test_data[['text']]]\n",
    "\n",
    "#check the result\n",
    "print(\"Test length:\",len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "#more info on training dataset\n",
    "\n",
    "print(Y_train[:30])\n",
    "def convert_class(raw):\n",
    "    if raw == 'negative': return 0\n",
    "    elif raw == 'neutral': return 1\n",
    "    elif raw == 'positive': return 2\n",
    "\n",
    "\n",
    "Y_train_num = []\n",
    "for y in Y_train:\n",
    "    Y_train_num.append(convert_class(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_raw, Y_train, test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross fold validation to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6697752637211436\n",
      "0.5966977526372115\n"
     ]
    }
   ],
   "source": [
    "# Tfidf Vectorizer\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)\n",
    "\n",
    "model_lr = LogisticRegression(random_state=0, multi_class='ovr', solver = 'liblinear')\n",
    "model_lr.fit(tf_x_train,y_train)\n",
    "print(model_lr.score(tf_x_test, y_test))\n",
    "\n",
    "mnb = MultinomialNB().fit(tf_x_train,y_train)\n",
    "print(mnb.score(tf_x_test, y_test))\n",
    "\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Tarin dataset using TFIDF\n",
    "#X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "#Use the feature set (vocabulary) from Train to vectorise the Test dataset \n",
    "#X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6705396728329002\n",
      "0.6777251184834123\n"
     ]
    }
   ],
   "source": [
    "# BoW + Kbest\n",
    "BoW_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_BoW = BoW_vectorizer.fit_transform(X_train)\n",
    "X_test_BoW = BoW_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "kbest = SelectKBest(chi2, k=1000).fit(X_train_BoW, y_train)\n",
    "kbest_train_X = kbest.transform(X_train_BoW)\n",
    "kbest_test_X = kbest.transform(X_test_BoW)\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Tarin dataset using BoW\n",
    "#X_train_BoW = BoW_vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "#Use the feature set (vocabulary) from Train to vectorise the Test dataset \n",
    "#X_test_BoW = BoW_vectorizer.transform(X_test_raw)\n",
    "\n",
    "\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(kbest_train_X,y_train)\n",
    "\n",
    "print(clf.score(kbest_test_X, y_test))\n",
    "\n",
    "model_lr.fit(kbest_train_X,y_train)\n",
    "print(model_lr.score(kbest_test_X, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the created vocabulary for the given dataset in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34424, 2)\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = BoW_vectorizer.vocabulary_\n",
    "output_pd = pd.DataFrame(list(vocab_dict.items()),columns = ['word','count'])\n",
    "print(output_pd.shape)\n",
    "\n",
    "vocab_dict\n",
    "output_pd.T.to_csv('BoW-vocab.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mutual_info_classif\n\u001B[1;32m      5\u001B[0m mi \u001B[38;5;241m=\u001B[39m SelectKBest(score_func\u001B[38;5;241m=\u001B[39mmutual_info_classif, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m X_train_mi \u001B[38;5;241m=\u001B[39m \u001B[43mmi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_BoW\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m X_test_mi \u001B[38;5;241m=\u001B[39m mi\u001B[38;5;241m.\u001B[39mtransform(X_test_BoW)\n\u001B[1;32m      9\u001B[0m models \u001B[38;5;241m=\u001B[39m [\u001B[38;5;66;03m#GaussianNB(),\u001B[39;00m\n\u001B[1;32m     10\u001B[0m           MultinomialNB(),\n\u001B[1;32m     11\u001B[0m           DecisionTreeClassifier(max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m           SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m'\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m),\n\u001B[1;32m     16\u001B[0m           SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpoly\u001B[39m\u001B[38;5;124m'\u001B[39m, degree\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)]\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:855\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    852\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m--> 855\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:408\u001B[0m, in \u001B[0;36m_BaseFilter.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score function should be a callable, \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) was passed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscore_func, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscore_func))\n\u001B[1;32m    405\u001B[0m     )\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params(X, y)\n\u001B[0;32m--> 408\u001B[0m score_func_ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(score_func_ret, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscores_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpvalues_ \u001B[38;5;241m=\u001B[39m score_func_ret\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_mutual_info.py:464\u001B[0m, in \u001B[0;36mmutual_info_classif\u001B[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;124;03m\"\"\"Estimate mutual information for a discrete target variable.\u001B[39;00m\n\u001B[1;32m    391\u001B[0m \n\u001B[1;32m    392\u001B[0m \u001B[38;5;124;03mMutual information (MI) [1]_ between two random variables is a non-negative\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;124;03m       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\u001B[39;00m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    463\u001B[0m check_classification_targets(y)\n\u001B[0;32m--> 464\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_estimate_mi\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscrete_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_mutual_info.py:300\u001B[0m, in \u001B[0;36m_estimate_mi\u001B[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001B[0m\n\u001B[1;32m    297\u001B[0m     y \u001B[38;5;241m=\u001B[39m scale(y, with_mean\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    298\u001B[0m     y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-10\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mmaximum(\u001B[38;5;241m1\u001B[39m, np\u001B[38;5;241m.\u001B[39mmean(np\u001B[38;5;241m.\u001B[39mabs(y))) \u001B[38;5;241m*\u001B[39m rng\u001B[38;5;241m.\u001B[39mrandn(n_samples)\n\u001B[0;32m--> 300\u001B[0m mi \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    301\u001B[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, discrete_feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(_iterate_columns(X), discrete_mask)\n\u001B[1;32m    303\u001B[0m ]\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(mi)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_mutual_info.py:301\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    297\u001B[0m     y \u001B[38;5;241m=\u001B[39m scale(y, with_mean\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    298\u001B[0m     y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-10\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mmaximum(\u001B[38;5;241m1\u001B[39m, np\u001B[38;5;241m.\u001B[39mmean(np\u001B[38;5;241m.\u001B[39mabs(y))) \u001B[38;5;241m*\u001B[39m rng\u001B[38;5;241m.\u001B[39mrandn(n_samples)\n\u001B[1;32m    300\u001B[0m mi \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 301\u001B[0m     \u001B[43m_compute_mi\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscrete_feature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscrete_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, discrete_feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(_iterate_columns(X), discrete_mask)\n\u001B[1;32m    303\u001B[0m ]\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(mi)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_mutual_info.py:160\u001B[0m, in \u001B[0;36m_compute_mi\u001B[0;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;124;03m\"\"\"Compute mutual information between two variables.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \n\u001B[1;32m    156\u001B[0m \u001B[38;5;124;03mThis is a simple wrapper which selects a proper function to call based on\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;124;03mwhether `x` and `y` are discrete or not.\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x_discrete \u001B[38;5;129;01mand\u001B[39;00m y_discrete:\n\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmutual_info_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m x_discrete \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m y_discrete:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _compute_mi_cd(y, x, n_neighbors)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/cluster/_supervised.py:783\u001B[0m, in \u001B[0;36mmutual_info_score\u001B[0;34m(labels_true, labels_pred, contingency)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m contingency \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     labels_true, labels_pred \u001B[38;5;241m=\u001B[39m check_clusterings(labels_true, labels_pred)\n\u001B[0;32m--> 783\u001B[0m     contingency \u001B[38;5;241m=\u001B[39m \u001B[43mcontingency_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    784\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    785\u001B[0m     contingency \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[1;32m    786\u001B[0m         contingency,\n\u001B[1;32m    787\u001B[0m         accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoo\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    788\u001B[0m         dtype\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39mint32, np\u001B[38;5;241m.\u001B[39mint64],\n\u001B[1;32m    789\u001B[0m     )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/cluster/_supervised.py:137\u001B[0m, in \u001B[0;36mcontingency_matrix\u001B[0;34m(labels_true, labels_pred, eps, sparse, dtype)\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot set \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when sparse=True\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    136\u001B[0m classes, class_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(labels_true, return_inverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 137\u001B[0m clusters, cluster_idx \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m n_classes \u001B[38;5;241m=\u001B[39m classes\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    139\u001B[0m n_clusters \u001B[38;5;241m=\u001B[39m clusters\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36munique\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/arraysetops.py:262\u001B[0m, in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001B[0m\n\u001B[1;32m    260\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/arraysetops.py:320\u001B[0m, in \u001B[0;36m_unique1d\u001B[0;34m(ar, return_index, return_inverse, return_counts)\u001B[0m\n\u001B[1;32m    317\u001B[0m optional_indices \u001B[38;5;241m=\u001B[39m return_index \u001B[38;5;129;01mor\u001B[39;00m return_inverse\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m optional_indices:\n\u001B[0;32m--> 320\u001B[0m     perm \u001B[38;5;241m=\u001B[39m \u001B[43mar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margsort\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmergesort\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquicksort\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# adapted from workshop 9\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_mi = mi.fit_transform(X_train_BoW,y_train)\n",
    "X_test_mi = mi.transform(X_test_BoW)\n",
    "\n",
    "models = [#GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          DecisionTreeClassifier(max_depth=1),\n",
    "          KNeighborsClassifier(n_neighbors=5),\n",
    "          DecisionTreeClassifier(max_depth=None),\n",
    "          LogisticRegression(max_iter = 1000),\n",
    "          SVC(kernel='rbf', gamma=0.7),\n",
    "          SVC(kernel='poly', degree=3)]\n",
    "titles = [#'GNB',\n",
    "          'MNB',\n",
    "          'one-r',\n",
    "          '5-nearest neighbour',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression',\n",
    "          'SVM with a cubic kernel',\n",
    "          'SVM with an RBF kernel']\n",
    "\n",
    "k = 1000\n",
    "\n",
    "for k in [100,1000,10000]: #10,\n",
    "    print('\\n--------------------------------------- K = ', k,'------------------------------')\n",
    "    x2 = SelectKBest(chi2, k=k)\n",
    "    x2.fit(X_train_BoW,y_train)\n",
    "    X_train_x2 = x2.transform(X_train_BoW)\n",
    "    X_test_x2 = x2.transform(X_test_BoW)\n",
    "\n",
    "\n",
    "    mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    mi.fit(X_train_BoW,y_train)\n",
    "    X_train_mi = mi.transform(X_train_BoW)\n",
    "    X_test_mi = mi.transform(X_test_BoW)\n",
    "\n",
    "\n",
    "    Xs = [(X_train_BoW, X_test_BoW), (X_train_x2, X_test_x2), (X_train_mi, X_test_mi)]\n",
    "    X_names = ['complete', 'x2', 'mi']\n",
    "    for title, model in zip(titles, models):\n",
    "        print('\\n=========',title, '(with k=',k,'features): ')\n",
    "        \n",
    "        for X_name, X in zip(X_names, Xs):\n",
    "            X_train_t, X_test_t = X\n",
    "            \n",
    "            model.fit(X_train_t.todense(), y_train)\n",
    "            y_test_predict = model.predict(X_test_t.todense())\n",
    "            accuracy =  accuracy_score(y_test, y_test_predict)\n",
    "            print(X_name, 'accuracy is:',  accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------- K =  10 ------------------------------\n",
    "\n",
    "========= GNB (with k= 10 features):\n",
    "complete accuracy is: 0.4204250114661367\n",
    "x2 accuracy is: 0.26968353462773276\n",
    "mi accuracy is: 0.2761045711664883\n",
    "\n",
    "========= MNB (with k= 10 features): \n",
    "complete accuracy is: 0.6584620088671457\n",
    "x2 accuracy is: 0.592722825256077\n",
    "mi accuracy is: 0.5982265708607246\n",
    "\n",
    "========= one-r (with k= 10 features): \n",
    "complete accuracy is: 0.5965448708148601\n",
    "x2 accuracy is: 0.5965448708148601\n",
    "mi accuracy is: 0.5965448708148601\n",
    "\n",
    "========= 5-nearest neighbour (with k= 10 features): \n",
    "complete accuracy is: 0.5863017887173215\n",
    "x2 accuracy is: 0.5852316159608623\n",
    "mi accuracy is: 0.5849258523161596\n",
    "\n",
    "========= Decision Tree (with k= 10 features): \n",
    "complete accuracy is: 0.5685674973245681\n",
    "x2 accuracy is: 0.6116801712276411\n",
    "mi accuracy is: 0.6112215257605871\n",
    "\n",
    "========= Logistic Regression (with k= 10 features): \n",
    "complete accuracy is: 0.6606023543800642\n",
    "x2 accuracy is: 0.6144320440299649\n",
    "mi accuracy is: 0.6141262803852622\n",
    "\n",
    "========= SVM with a cubic kernel (with k= 10 features): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Adapting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC().fit(tf_x_train, y_train)\n",
    "\n",
    "#cvscvore(svc, tf_x_train, y_train, cv=5). mean0, sve.score (tf_x_test, y_test)\n",
    "print(len(y_train))\n",
    "svc_pred = svc.predict(tf_x_test)\n",
    "print(classification_report(y_test, svc_pred))\n",
    "matrix_svc = confusion_matrix(y_test, svc_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(matrix_svc, cmap = 'Blues', annot = True)\n",
    "plt.xlabel (\"Predicted classes\")\n",
    "plt.ylabel (\"Actual classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(kbest_train_X,y_train)\n",
    "\n",
    "print(clf.score(kbest_test_X, y_test))\n",
    "\n",
    "model_lr.fit(kbest_train_X,y_train)\n",
    "print(model_lr.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "# store best combination as x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "bagging = BaggingClassifier().fit(kbest_train_X, y_train)\n",
    "print(bagging.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "#voting\n",
    "base_classifiers = [('dt', DecisionTreeClassifier()),\n",
    "                    (' 1r', LogisticRegression (C= 0.5)),\n",
    "                    ('mnb' , MultinomialNB()),\n",
    "                    (' svc', LinearSVC (C=1))]\n",
    "voting = VotingClassifier (estimators=base_classifiers)\n",
    "\n",
    "voting.fit(kbest_train_X ,y_train)\n",
    "print(voting.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "#random forest\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators=100, verbose=1).fit(kbest_train_X, y_train)\n",
    "print(rf.score(kbest_test_X, y_test))\n",
    "\n",
    "\n",
    "#ada\n",
    "ada = AdaBoostClassifier().fit(kbest_train_X,y_train)\n",
    "print(ada.score(kbest_test_X, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final prediction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}